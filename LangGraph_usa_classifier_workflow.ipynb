{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec0ae86",
   "metadata": {},
   "source": [
    "# üìò USA Classifier Workflow with LangGraph\n",
    "\n",
    "This notebook demonstrates a simple decision-based agent workflow using LangGraph. It includes:\n",
    "- A supervisor node to classify questions\n",
    "- A router function to decide between RAG or LLM\n",
    "- Two tool nodes (RAG and LLM)\n",
    "\n",
    "We'll walk through the data preparation, model setup, and LangGraph orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c9c88",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 1: Load Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f51518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--4f72d9c3-58af-4406-8935-117addebafd0-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "output = model.invoke(\"Hi\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40b0315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "len(embeddings.embed_query(\"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403275b",
   "metadata": {},
   "source": [
    "## üìÑ Step 2: Load and Embed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ac68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(\"/Users/ankita/Documents/Krish Naik Academy/Agentic Batch 2/3-Langraph/data\", glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "new_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6dace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/ankita/Documents/Krish Naik Academy/Agentic Batch 2/3-Langraph/data/usa.txt'}, page_content='Looking forward, the U.S. economy is expected to grow at a moderate pace, powered by innovation in AI, green energy, robotics, biotech, and quantum computing. The Biden administration‚Äôs Inflation'),\n",
       " Document(metadata={'source': '/Users/ankita/Documents/Krish Naik Academy/Agentic Batch 2/3-Langraph/data/usa.txt'}, page_content='üá∫üá∏ Overview of the U.S. Economy'),\n",
       " Document(metadata={'source': '/Users/ankita/Documents/Krish Naik Academy/Agentic Batch 2/3-Langraph/data/usa.txt'}, page_content='The U.S. economy remains the engine of global growth, backed by unmatched innovation, financial dominance, and a strong institutional framework. Its $28 trillion GDP and influence over global')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever.invoke(\"industrial growth of usa?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212b080",
   "metadata": {},
   "source": [
    "## üß± Step 3: Define Agent State and Pydantic Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0106902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description=\"selected topic\")\n",
    "    Reasoning: str = Field(description=\"Reasoning behind topic selection\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be69a81",
   "metadata": {},
   "source": [
    "## üß† Step 4: Supervisor Node (Topic Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa1f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def function_1(state: AgentState):\n",
    "    question = state[\"messages\"][-1]\n",
    "    print(\"Question:\", question)\n",
    "\n",
    "    template = '''\n",
    "    Your task is to classify the given user query into one of the following categories: [USA, Not Related].\n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variable=[\"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "    chain = prompt | model | parser\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(\"Parsed response:\", response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010a72c",
   "metadata": {},
   "source": [
    "## üîÄ Step 5: Router Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "636ec2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: AgentState):\n",
    "    print(\"-> ROUTER ->\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"usa\" in last_message.lower():\n",
    "        return \"RAG Call\"\n",
    "    else:\n",
    "        return \"LLM Call\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962deb1",
   "metadata": {},
   "source": [
    "## üìö Step 6: RAG Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d80c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def function_2(state: AgentState):\n",
    "    print(\"-> RAG Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = rag_chain.invoke(question)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be5a84",
   "metadata": {},
   "source": [
    "## ü§ñ Step 7: LLM Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705dec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(state: AgentState):\n",
    "    print(\"-> LLM Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    complete_query = \"Answer the following question with your real-world knowledge: \" + question\n",
    "    response = model.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a46d9",
   "metadata": {},
   "source": [
    "## üîÅ Step 8: Build LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77712197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Supervisor\", function_1)\n",
    "workflow.add_node(\"RAG\", function_2)\n",
    "workflow.add_node(\"LLM\", function_3)\n",
    "workflow.set_entry_point(\"Supervisor\")\n",
    "\n",
    "workflow.add_conditional_edges(\"Supervisor\", router, {\n",
    "    \"RAG Call\": \"RAG\",\n",
    "    \"LLM Call\": \"LLM\"\n",
    "})\n",
    "\n",
    "workflow.add_edge(\"RAG\", END)\n",
    "workflow.add_edge(\"LLM\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6f75d",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Run Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "291c629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the GDP of USA?\n",
      "Parsed response: Topic='USA' Reasoning='The query explicitly asks for the GDP of the USA.'\n",
      "-> ROUTER ->\n",
      "-> RAG Call ->\n",
      "‚úÖ Final Answer: The nominal GDP of the USA is estimated to be around $28 trillion USD as of 2024.  This represents about 25% of the global economy.  The US has the world's largest nominal GDP.\n"
     ]
    }
   ],
   "source": [
    "state = {\"messages\": [\"What is the GDP of USA?\"]}\n",
    "result = app.invoke(state)\n",
    "print(\"‚úÖ Final Answer:\", result[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4b647",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, you:\n",
    "- Created a LangGraph state machine with Supervisor, RAG, and LLM nodes\n",
    "- Used a router to decide paths based on classification\n",
    "- Connected it to Chroma vectorstore for retrieval\n",
    "\n",
    "### ‚ùì Is it an Agent?\n",
    "Yes ‚úÖ ‚Äî it qualifies as a **basic decision-based agent**. The supervisor + routing + tool calling behavior is agentic, but not yet fully reactive or memory-enhanced.\n",
    "\n",
    "### We use PydanticOutputParser in LangChain when:\n",
    "* You want structured output from an LLM\n",
    "* You want type-safe access to fields (like Topic, Reasoning, Summary, etc.)\n",
    "* You want to validate the output against a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf6fa0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
